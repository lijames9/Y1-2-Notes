\documentclass[10pt, letterpaper, twoside]{article}
\usepackage[legalpaper, portrait, margin=0.7in]{geometry}
\usepackage{soul}
\usepackage[norule]{footmisc}
\usepackage{multicol,caption}
\usepackage{sectsty}
\usepackage{graphicx}
\usepackage{titling}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\renewcommand{\thesection}{\Roman{section}} 
\renewcommand{\thesubsection}{\thesection.\Roman{subsection}}
\graphicspath{ {./images/} }
\usepackage{amsfonts}
\usepackage{array}
\usepackage{tabu}
\usepackage[table]{xcolor}
\usepackage{pdflscape}
\usepackage{makecell}
\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.7}
\usepackage{tikz,lipsum,lmodern}
\usepackage[most]{tcolorbox}
\usepackage{subfiles}
\usepackage{afterpage}
\usepackage{filecontents}
\usepackage{setspace}
\usepackage{caption}
\usepackage{minted}

\DeclareCaptionType{equ}[][]
\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}

\setlength{\parindent}{1em}
\setlength{\parskip}{0.9em}

\sectionfont{\fontsize{11}{15}\selectfont}
\subsectionfont{\fontsize{11}{15}\selectfont}
\setlength{\columnsep}{1cm}

\title{\textbf{Non-Comprehensive ECE286 Notes}}
\author{James Li}
\date{\today}

\begin{document}


\maketitle
\begin{multicols}{2}

\section{Counting and Other Shenanigans}

\textbf{Permutations:} cares about order; given n items, no. of permutations is n!
\begin{itemize}
    \item no. of permutations of r out n items:
        \begin{equation*}
                \frac{n!}{(n-r)!}
        \end{equation*}
    \item with repeated items:
        \begin{equation*}
            \binom{n}{n_1, ..., n_m} = \frac{n!}{n_1!...n_m!}\hspace{0.25cm}; \hspace{0.5cm} \sum_{k=1}^mn_k = n
        \end{equation*}
    \item Example: permutations of ATLANTIC:
        \begin{equation*}
            \binom{8}{2, 2, 1, 1, 1, 1}= \frac{8!}{2!2!}
        \end{equation*}
\end{itemize}
\textbf{Partitions:} Events that are mutually exclusive; found the same as permutations with repetition:
\begin{equation*}
    \binom{n}{n_1, ..., n_m} = \frac{n!}{n_1!...n_m!}
\end{equation*}
\textbf{Combinations:} does not care about order; expressed as $\binom{n}{r}$
\begin{itemize}
    \item \textbf{size r combination:} partition with n$_1$ = r, n$_2$ = n-r:
    \begin{equation*}
        \binom{n}{r, n-r} = \frac{n!}{r!(n-r)!}
    \end{equation*}
\end{itemize}
\section{Probability}
\begin{itemize}
    \item Given that A, B $\subseteq$ S: if A $\cap$ B = $\null$, $\mathbb{P}$(A) + $\mathbb{P}$(B) = 1.
\end{itemize}

\textbf{Additive Rule:} Given that A, B $\subseteq$ S:

\begin{equation*}
    \mathbb{P}(A\cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A\cap B)
\end{equation*}
\begin{itemize}
    \item for 3 events, given A, B, C $\subseteq$ S, :
\end{itemize}
\begin{equation*}
\begin{split}
    \mathbb{P}(A \cup B \cup C) = \mathbb{P}(A) + \mathbb{P}(B) + \mathbb{P}(C) - \mathbb{P}(A\cap B)\\ - \mathbb{P}(A\cap C) - \mathbb{P}(B\cap C) + \mathbb{P}(A\cap B \cap C)
\end{split}
\end{equation*}

\textbf{Conditional Probability:} denoted as $\mathbb{P}$(B|A); probability that event B occurs given A occurs first.

\begin{equation*}
    \mathbb{P}(B|A) = \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}
\end{equation*}

\textbf{Product Rule:} $\mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B|A)$

\textbf{Independence:} Event A is independent of B is $\mathbb{P}$(A|B) = $\mathbb{P}$(A), $\mathbb{P}$(B|A) = $\mathbb{P}$(B)
\begin{itemize}
    \item not the same as mutually exclusive -- A $\cap$ B = $\emptyset$
    \item $\mathbb{P}$(A $\cap$ B) = $\mathbb{P}$(A)$\mathbb{P}$(B)
\end{itemize}
\textbf{Bayes' Rule:} Utilizes the Product Rule and Conditional Probability to derive this equality:

\begin{equation*}
    \frac{\mathbb{P}(A|B)}{\mathbb{P}(A)} = \frac{\mathbb{P}(B|A)}{\mathbb{P}(B)}
\end{equation*}
\section{Total Probability}

\begin{itemize}
    \item Suppose A is an event, B$_1$...B$_k$ is a partition, then:
    \begin{equation*}
        \mathbb{P}(A) = \sum_{i=1}^k\mathbb{P}(A\cap B_i) = \sum_{i=1}^k\mathbb{P}(A|B_i)\mathbb{P}(B_i)
    \end{equation*}
\end{itemize}
\textbf{Bayes' Rule (retooled):} suppose that C$_1$,..., C$_k$ is a partition:
\begin{equation*}
    \begin{split}
        \mathbb{P}(B|A) = \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)} = \frac{\mathbb{P}(B)\mathbb{P}(A|B)}{\mathbb{P}(A)} \\
        = \frac{\mathbb{P}(B)\mathbb{P}(A|B)}{\sum_{i=1}^k\mathbb{P}(C_i)\mathbb{P}(A|C_i)}
    \end{split}
\end{equation*}
\begin{itemize}
    \item and if B is an element of \{C$_1$,...,C$_k$\}:
    \begin{equation*}
        \mathbb{P}(C_n|A) = \frac{\mathbb{P}(C_n)\mathbb{P}(A|C_n)}{\sum_{i=1}^k\mathbb{P}(C_i)\mathbb{P}(A|C_i)}
    \end{equation*}
\end{itemize}
\vfill\pagebreak
\section{Random Variables}

\textbf{Random Variable:} Function that maps each element of a sample space to a real number:

\begin{itemize}
    \item 3 coin flips, RV X is no. of H:
    \begin{itemize}
        \item $\mathbb{P}$(X = 3) = 1/8
        \item $\mathbb{P}$(X = 2) = 3/8
    \end{itemize}
    \item \textbf{discrete ---} countable
    \item \textbf{continuous ---} in interval of $\mathbb{R}$
\end{itemize}
\subsection{Discrete Probability Distributions}
\textbf{Probability Mass Function (PMF)}
\begin{itemize}
    \item $\sum_x$ f(x) = 1, f(x) = $\mathbb{P}$(X=x), f(x) $\geq$ 0 for X = x
\end{itemize}
\textbf{Cumulative Distribution Function (CDF)}
\begin{itemize}
    \item If X has PMF f(x), CDF:
    \begin{equation*}
        F(x) = \sum_{t\leq x}f(t)
    \end{equation*}
    \item F(x) = $\mathbb{P}$(X$\leq$x); i.e) F(1) = f(0) + f(1)
\end{itemize}

\subsection{Continuous Probability Distributions}
\textbf{Probability Mass Function}
\begin{itemize}
    \item $\int_{-\infty}^{\infty}f(x)dx = 1$, $\int_a^b f(x)dx = \mathbb{P}(a < X < b)$  
\end{itemize}
\textbf{Cumulative Distribution Function}
\begin{itemize}
    \item F(x) = $\int_{-\infty}^xf(t)dt$, $\mathbb{P}(a < X\leq b)$ = F(b) - F(a)
    \item F($\infty$) = $\int_{-\infty}^{\infty}f(t)dt = 1$
\end{itemize}
\subsection{Joint Distributions}

\textbf{Joint Distribution:} a function $f(x,y)$ is a joint PMF of RVs X an Y if:

\begin{itemize}
    \item $f(x,y) \geq 0$ for all x, y.
    \item $\mathbb{P}(X = x, Y = y) = f(x,y)$
    \item \textbf{Discrete:} $\sum_x\sum_yf(x,y) = 1$
    \begin{itemize}
        \item Given A $\subset$ S:
        \begin{equation*}
            \mathbb{P}(X,Y \in A) = \sum_{(x,y)\in A}f(x,y)
        \end{equation*}
    \end{itemize}
    \item \textbf{Continuous:} $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y)dxdy = 1$
    \begin{itemize}
        \item Given A $\subset$ S:
        \begin{equation*}
            \mathbb{P}(X,Y \in A) = \int_{(x,y)\in A}f(x,y)dxdy
        \end{equation*}
    \end{itemize}
\end{itemize}

\subsubsection{Marginal Distributions}

A \textbf{marginal distribution} is the distribution of each individual RV given a joint distribution, i.e ) g(x) is the marginal distribution of X.

\begin{itemize}
    \item \textbf{Discrete:}
    \begin{equation*}
        g(x) = \sum_y f(x,y) \hspace{0.5 cm} h(y) = \sum_x f(x,y)
    \end{equation*}
    \item \textbf{Continuous:}
    \begin{equation*}
        g(x) = \int_{-\infty}^{\infty}f(x,y)dy \hspace{0.5cm} h(y) = \int_{-\infty}^{\infty}f(x,y)dx
    \end{equation*}
\end{itemize}

\subsubsection{Conditional Distributions}

Same idea as conditional probability, but applied to a PDF:

\begin{equation*}
    f(x|y) = \frac{f(x,y)}{g(y)}
\end{equation*}

where g(y) is the marginal distribution of y.
\begin{itemize}
    \item \textbf{Discrete:} 
    \begin{equation*}
        \mathbb{P}(a \leq X \leq b|Y=y) = \sum_{a\leq x\leq b}f(x|y)
    \end{equation*}
    \item \textbf{Continuous:}
    \begin{equation*}
        \mathbb{P}(a \leq X \leq b|Y = y) = \int_a^b f(x|y)dx
    \end{equation*}
\end{itemize}

Random Variables X and Y are \textbf{independent} if joint distribution $f(x,y)$ and marginal distributions $g(y), h(x)$ can be expressed as:
\begin{equation*}
    f(x,y) = g(y)h(x)
\end{equation*}
\vfill\pagebreak

\section{Expectation, Variance, Covariance}

Given RV X has a distribution $f(x)$, the \textbf{expected value (or mean), E[X]}:
\begin{itemize}
    \item \textbf{Discrete:}
    \begin{equation*}
        E[X] = \sum_xxf(x)
    \end{equation*}
    \item\textbf{Continuous:}
    \begin{equation*}
        E[X] = \int_{-\infty}^{\infty}xf(x)dx
    \end{equation*}
\end{itemize}

Given RV X has a distribution $f(x)$ and g(X) is a function of X, the expected value, E[g(X)]:
\begin{itemize}
    \item \textbf{Discrete:}
    \begin{equation*}
        E[g(X)] = \sum_xg(x)f(x)
    \end{equation*}
    \item\textbf{Continuous:}
    \begin{equation*}
        E[g(X)] = \int_{-\infty}^{\infty}g(x)f(x)dx
    \end{equation*}
\end{itemize}

Given RVs X and Y have a joint distribution $f(x,y)$ and g(X,Y) is a function of X and Y, the expected value, E[g(X,Y)]:
\begin{itemize}
    \item \textbf{Discrete:}
    \begin{equation*}
        E[g(X,Y)] = \sum_x\sum_yg(x,y)f(x,y)
    \end{equation*}
    \item\textbf{Continuous:}
    \begin{equation*}
        E[g(X,Y)] = \int_{-\infty}^{\infty}g(x,y)f(x,y)dx
    \end{equation*}
\end{itemize}

Let X be an RV with distribution f(x) and mean $\mu$ = E[X]. The \textbf{variance} of X, $\sigma^2$, is:
\begin{itemize}
    \item \textbf{Discrete:}
        \begin{equation*}
        \sigma^2 = E[(x-\mu)^2] = \sum_x(x-\mu)^2f(x)
        \end{equation*}
    \item \textbf{Continuous:}
        \begin{equation*}
            \sigma^2 = \int_{-\infty}^{\infty}(x-\mu)^2f(x)dx
        \end{equation*}
    \item $\sigma$ is known as the \textbf{standard deviation}.
    \item \textbf{Useful Formula:}
    \begin{equation*}
        \sigma^2 = E[X^2]- \mu^2 = E[X^2] - E[X]^2
    \end{equation*}
\end{itemize}

Let X and Y be RVs with joint distribution f(x, y) and means $\mu_X$ and $\mu_Y$. The \textbf{covariance}, $\sigma_{XY}$, of X and Y is:

\begin{itemize}
    \item \textbf{Discrete:}
        \begin{equation*}
            \sigma_{XY} = \sum_x\sum_y(x-\mu_X)(y-\mu_Y)f(x,y)
        \end{equation*}
    \item \textbf{Continuous:}
        \begin{equation*}
            \sigma_{XY} = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x-\mu_X)(y-\mu_Y)f(x,y) dxdy
        \end{equation*}
    \item if X>0, Y>0, $\sigma_{XY}$ > 0; if X>0, Y<0, $\sigma_{XY} < 0$.
    \item \textbf{Useful Formula:}
    \begin{equation*}
        \sigma_{XY} = E[XY] - \mu_X\mu_Y
    \end{equation*}
\end{itemize}

Let X and Y be RVs with covariance $\sigma_{XY}$ and standard deviations $\sigma_X$ and $\sigma_Y$. The \textbf{correlation coefficient} of X and Y is:

\begin{equation*}
    \rho_{XY} = \frac{\sigma_{XY}}{\sigma_X\sigma_Y}, -1\leq\rho_{XY} \leq 1
\end{equation*}

\subsection{Linear Combinations}
\textbf{Expectation Value:}
\begin{itemize}
    \item The expectation of the RV $aX+Y$ is:
    \begin{equation*}
       E[aX + Y] = aE[X] + E[Y]
    \end{equation*}
    \item Similarly:
    \begin{align*}
        E[aX + b] &= aE[X] + b\\
        E[g(X,Y) + h(X,Y)] &= E[g] + E[h]
    \end{align*}
\end{itemize}

\textbf{Variance, Covariance:} Suppose X and Y are independent \textbf{---} $f(x,y) = g(x)h(y)$:
\begin{equation*}
    E[XY] = E[X]E[Y]
\end{equation*}
\textbf{Recall: covariance}
\begin{equation*}
    \sigma_{XY} = E[XY] - E[X]E[Y]
\end{equation*}
\begin{itemize}
    \item Independence implies \textbf{uncorrelated} ($\sigma_{XY} = 0$)
    \item Uncorrelated does not imply independence.
\end{itemize}
\textbf{Example:} Consider the following PDF.
\begin{equation*}
    f(x,y) =
    \begin{cases}
        1/\pi, \hspace{0.1cm} x^2 + y^2 = 1 \\
        0, \hspace{0.1cm} \text{else}
    \end{cases}
\end{equation*}
\begin{itemize}
    \item Check for the function's independence:
    \begin{align*}
        g(x) &= \frac{2}{\pi}\sqrt{1-x^2}\\
        h(y) &= \frac{2}{\pi}\sqrt{1-y^2}
    \end{align*}
    \item Note that $g(x)h(y) \neq f(x,y)$, implying RVs X and Y are not independent.
    \item Check for the covariance of the function:
    \begin{equation*}
        E[XY] = E[X] = E[Y] = 0
    \end{equation*}
    \item Thus, the RVs are uncorrelated but not independent.
    \item \textbf{Useful formula:}
    \begin{equation*}
        \sigma^2_{aX+bY+c} = a^2\sigma_X^2 + b^2\sigma_Y^2 + 2ab\sigma^2_{XY}
    \end{equation*}
\end{itemize}
\vfill\pagebreak

\section{Distributions}

A \textbf{uniform distribution} implies that every element in S has the same probability.
\begin{itemize}
    \item If S = \{1,..., n\}, $f(k) = \frac{1}{n}$ given k $\in$ S.
\end{itemize}

A \textbf{Bernoulli distribution} is a probability distribution with 2 outcomes, where the probability of achieving a 1 is $\mathbf{p}$ and a 0 is $\mathbf{q = 1 - p}$.

\textbf{Binomial Distribution/Bernoulli Process:} suppose a binomial event is repeated n times, and RV X is the number of 1's that occurs:

\begin{itemize}
    \item Notation for binomial distribution: $f(x) = b(x;n,p)$
    \item Probability of x 1's and n-x 0's in a particular order:
    \begin{equation*}
        p^x(1-p)^{n-x}
    \end{equation*}
    \begin{equation*}
        b(x;n,p) = \binom{n}{x}p^x(1-p)^{n-x}
    \end{equation*}
\end{itemize}

\textbf{Expectation value} of a binomial distribution:

\begin{equation*}
    E[X] = \sum_{x=0}^nx\binom{n}{x}p^x(1-p)^{n-x}
\end{equation*}
\begin{itemize}
    \item Picture Bernoulli as a sum of n trials, and apply linearity:
    \begin{align*}
        E[X] &= E[Y_1] + ... + E[Y_n] \\
        &= np
    \end{align*}
\end{itemize}

\textbf{Variance}:
\begin{align*}
    \sigma_X^2 &= \sum_{k=1}^n\sigma_{Y_k}^2\\
    &= np(1-p)
\end{align*}

A \textbf{Multinomial distribution} is the same idea as a binomial distribution but each trial can now have m outcomes rather than 2 outcomes.

\begin{itemize}
    \item The chance of any outcome $i$, $\mathbb{P}(E_i)$:
    \begin{equation*}
        \mathbb{P}(E_i) = p_i, \hspace{0.25cm}\sum_{i=1}^mp_i = 1
    \end{equation*}
    \item The multinomial distribution relates the probability of outcome i happening x$_i$ times, $p_i^{x_i}$, with the concept as repeated items in a permutation, or a partition:
    \begin{equation*}
        f(x_1...x_n;p_1...p_m, n) = \binom{n}{x_1, ..., x_m}p_1^{x_1}...p_m^{x_m}
    \end{equation*}
\end{itemize}

A \textbf{Hypergeometric distribution} is different in the sense that there is \hl{\textbf{no replacement}} after the object is drawn.

\begin{itemize}
    \item Given N objects with K successes drawn n times, the chance of having x successes and n-x failures is:
    \begin{equation*}
        h(x;N,n,K) = \frac{\binom{K}{x}\binom{N-K}{n-x}}{\binom{N}{n}}
    \end{equation*}
    \item If N can be partitioned into events $a_1...a_k$, the \textbf{multivariate hypergeometric distribution} is as follows:
    \begin{equation*}
        f(x_1...x_k;a_1...a_k, N, n) =\frac{\binom{a_1}{x_1}...\binom{a_k}{x_k}}{\binom{N}{n}}
    \end{equation*}
    \item The mean and variance for a hypergeometric distribution is:
    \begin{equation*}
        \mu = \frac{nK}{N},\hspace{0.25cm}\sigma^2 = \frac{Kn(N-n)}{N(N-1)}(1-\frac{K}{N})
    \end{equation*}
    \end{itemize}
A \textbf{Negative Binomial} is the chance that the $k^{th}$ event occurs on the $n^{th}$ draw, denoted as $b^{*}(x;k,p)$.

\begin{equation*}
    b^{*}(x;k,p) = \binom{x-1}{k-1}p^k(1-p)^{x-k}
\end{equation*}

\begin{itemize}
    \item How to relate to binomial distribution?\footnote[1]{Check p.44 of Taylor's notes for proof}
    \begin{equation*}
        b^{*}(x;k,p) = pb(k-1;x-1,p)
    \end{equation*}
\end{itemize}

A \textbf{Geometric Distribution} is a negative binomial distribution with k = 1; when the first success occurs:

\begin{equation*}
    g(x;p) = b^{*}(x;1,p) = p(1-p)^{x-1}
\end{equation*}

\begin{itemize}
    \item The mean and variance of the geometric distribution are:
    \begin{equation*}
        \mu = \frac{1}{p},\hspace{0.25cm} \sigma^2 = \frac{1-p}{p^2}
    \end{equation*}
\end{itemize}

A \textbf{Poisson distribution} is the number of times that something happens in one sequence of intervals, $i.e)$ number of snow days in a year.

\begin{equation*}
    p(x;\lambda) = \frac{e^{-\lambda}\lambda^x}{x!}
\end{equation*}

\begin{itemize}
    \item The mean and variance of a Poisson distribution are both $\lambda$.
    \item As such, we can conclude that $\lambda$ is the average number of occurances per interval, which can be represented with r (rate) and t (length of interval):
    \begin{equation*}
        \lambda = rt = E[X]
    \end{equation*}
    \item How to represent a Poisson distribution with a binomial distribution\footnote[2]{Check p.48 of Taylor's notes for proof}:
    \begin{equation*}
        p(x;\lambda) = \lim_{n\rightarrow\infty,p\rightarrow 0}b(x;n,p)
    \end{equation*}
\end{itemize}

A \textbf{uniform distribution} for a continuous, uniform RV in [A, B] is:
\begin{equation*}
    f(x;A,B) = \begin{cases}
        \frac{1}{B-A}, \hspace{0.1cm} A \leq x \leq B \\
        0, \hspace{0.4cm} else
    \end{cases}
\end{equation*}
\begin{itemize}
    \item The mean and variance of a uniform distribution are: 
    \begin{equation*}
        \mu = \frac{A + B}{2}, \hspace{0.25 cm} \sigma^2 = \frac{(B-A)^2}{12}
    \end{equation*}
\end{itemize}

A \textbf{Gaussian (Normal) distribution}\footnote[3]{Is it a PDF? Check p.50. } for a normal RV X with mean $\mu$ and variance $\sigma^2$ is:
\begin{equation*}
    n(x;\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma}}e^{-{\frac{(x-\mu)^2}{2\sigma^2}}}, \hspace{0.1cm} -\infty < x < \infty
\end{equation*}
\end{multicols}

\begin{figure}[!ht]
    \centering
    \includegraphics[scale = 0.6]{pics/gauss 1.png} \includegraphics[scale = 0.6]{pics/gauss 2.png}
    \includegraphics[scale = 0.6]{pics/gauss 3.png}
    \caption{\textbf{Top left:} different means, same std dev; \textbf{Top right:} same mean, different std dev; \textbf{Bottom:} different means and std dev.}
    \label{fig:my_label}
\end{figure}

\begin{multicols}{2}
    

A \textbf{Standard normal distribution} is a Gaussian distribution where the mean is 0 and the standard deviation is 1:
\begin{equation*}
\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^{\frac{-t^2}{2}}dt
\end{equation*}

\begin{itemize}
    \item If X is a standard normal RV:
    \begin{equation*}
        P(A\leq X\leq B) = \frac{1}{\sqrt{2\pi}}\int_{A}^Be^{\frac{-t^2}{2}}dt = \Phi(B) - \Phi(A)
    \end{equation*}
    \item If X has PDF $n(x;\mu,\sigma)$, there is no analytical form for the CDF integral and it is inconvenient to compute $\mu$ and $\sigma$. 
    \item Instead, choose Z = $\frac{X - \mu}{\sigma}$, where Z has PDF $n(z;0,1)$:
    \begin{equation*}
        P(X\leq x) = \int_{-\infty}^xn(t;\mu,\sigma)dt = P(Z\leq\frac{x-\mu}{\sigma})
    \end{equation*}
    \item Similarly:
    \begin{equation*}
        P(A\leq X\leq B) = \Phi(\frac{B-\mu}{\sigma}) - \Phi(\frac{A-\mu}{\sigma})
    \end{equation*}
\end{itemize}

The \textbf{Normal approximation of a binomial PDF} can be represented as:
\begin{equation*}
    Z = \frac{X - np}{\sqrt{np(1-p)}}
\end{equation*}

\noindent noting that as $n \rightarrow \infty$, PDF of Z is $n(x;0,1)$.

\begin{itemize}
    \item Using the property $\mu = np$ and $\sigma = \sqrt{np(1-p)}$:
    \begin{equation*}
        P(X \leq x) = \sum_{k=0}^xb(k;n,p) = P(Z\leq\frac{(x+0.5) - \mu}{\sigma})
    \end{equation*}
    \item Note that for the lower z bound (z$_1$), you subtract 0.5 from the smaller x value, meaning $z_1 = \frac{x_1 - 0.5 - \mu}{\sigma}, z_2 = \frac{x_2 + 0.5 - \mu}{\sigma}$.
    \item If a question asks for the probability of \textbf{exactly}, still use normal approximation but with the same x value, meaning $z_1 = \frac{x - 0.5 - \mu}{\sigma}, z_2 = \frac{x + 0.5 - \mu}{\sigma}$.
\end{itemize}

\subsection{Gamma and Exponential Distributions}

The \textbf{gamma function}, $\Gamma(\alpha)$ is denoted as:
\begin{equation*}
    \Gamma(\alpha) = \int_0^{\infty}x^{\alpha-1}e^{-x}dx, \hspace{0.1cm} \alpha > 0
\end{equation*}

\noindent This gamma function is a generalization of the factorial function, where $\Gamma(n) = (n-1)!$ for $n \in \mathbb{N}$ and $\Gamma(\frac{1}{2}) = \sqrt{\pi}$.

For a RV X with \textbf{gamma distribution} with parameters $\alpha, \beta$ > 0:
\begin{equation*}
    f(x;\alpha,\beta) = \begin{cases}
        \frac{1}{\beta^{\alpha}\Gamma(\alpha)}x^{\alpha-1}e^{\frac{-x}{\beta}}, \hspace{0.2cm} x>0 \\
        0, \hspace{2cm} \text{else}
    \end{cases}
\end{equation*}

with $\mu = \alpha\beta$ and $\sigma^2 = \alpha\beta^2$.

\textbf{Special Cases of the Gamma Distribution} include:
\begin{itemize}
    \item \textbf{Chi-squared distribution} ($\chi^2$) with parameter v $\in \mathbb{N}$, where v is the number of degrees of freedom (in experiments, number of independent variables in calculation).
    \begin{equation*}
        f(x;v) = \begin{cases}
            \frac{1}{2^{\frac{v}{2}}\Gamma(\frac{v}{2})}x^{\frac{v}{2}-1}e^{-\frac{x}{2}}, \hspace{0.2cm} x>0\\
            0, \hspace{2cm} \text{else}
        \end{cases}
    \end{equation*}
    with $\mu = v, \sigma^2 = 2v$.
    \item \textbf{Exponential distribution} with parameter $\beta > 0$:
    \begin{equation*}
        f(x;\beta) = \begin{cases}
            \frac{1}{\beta}e^{\frac{-x}{\beta}}, \hspace{0.1cm} x>0\\
            0, \hspace{1cm} \text{else}
        \end{cases}
    \end{equation*}
    with $\alpha = 1, \mu = \beta, \sigma^2 = \beta^2$.
    \item \textbf{Poisson Process:} Consider the probability that no event occurs in t: $p(0;rt) = e^{-rt}$.
    \begin{itemize}
        \item Let X be the RV of the time to first event. $\mathbb{P}(X > x) = e^{-rx}$, and $\mathbb{P}(X \leq x) = 1 - e^{-rx}$.
        \item $\mathbb{P}(X \leq x)$ is a CDF ($F(x)$) and is the sum (integral) of a PDF ($f(x)$).
        \begin{equation*}
            f(x) = \frac{d}{dx}\mathbb{P}(X\leq x) = re^{-rx}
        \end{equation*}
        \noindent which is just the exponential distribution with r = 1/$\beta$.
    \end{itemize}
    \item Exponential distributions have a property called \textbf{the memoryless property}.
    \begin{equation*}
        P(X \geq s+t\hspace{0.05cm}|\hspace{0.05 cm} X \geq s) = P(X\geq t)
    \end{equation*}
\end{itemize}

\section{Functions of Random Variables}

\begin{itemize}
    \item Given X has PDF $f(x)$ and $Y = u(X)$, where $u(X)$ is a one-to-one function \textbf{---} each value of X maps one value to Y.
    \item As such, we can write $X = w(Y) = u^{-1}(Y)$. 
    \item \textbf{Discrete PDF:} the probability distribution of RV Y, if we let $g(y)$ be the distribution of Y:
    \begin{equation*}
        g(y) = f(u^{-1}(y))
    \end{equation*}
    
    \item \textbf{Continuous PDF:} the probability distribution of RV Y, if we let $g(y)$ be the distribution of Y:
    \begin{equation*}
        g(y) = f[w(y)]|J|
    \end{equation*}
    where |J| = $\frac{d(w(y))}{dy}$ is the Jacobian of the transformation.
\end{itemize}

Suppose X$_{1,2}$ are discrete random variables with joint PDF $f(x_1, x_2)$. If we let Y$_{1,2} = u_{1,2}(X_1, X_2)$, we can find an inverse expression $x_{1,2} = w_{1,2}(y_1, y_2)$.
\begin{itemize}
    \item \textbf{Joint Discrete PDF:} the joint probability distribution of RVs Y$_1$ and Y$_2$ is:
    \begin{equation*}
        g(y_1, y_2) = f(w_1(y_1, y_2), w_2(y_1, y_2))
    \end{equation*}
    \item \textbf{Joint Continuous PDF:} the joint probability distribution of RVs Y$_1$ and Y$_2$ is:
    \begin{equation*}
        g(y_1, y_2) = f(w_1(y_1, y_2), w_2(y_1, y_2))|J|
    \end{equation*}
    where |J| is the 2 $\times$ 2 determinant:
    \begin{equation*}
        J = \begin{vmatrix}
            \frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} \\
            \frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2}
        \end{vmatrix}
    \end{equation*}
\end{itemize}

If $Y = u(X)$ defines a transformation that is not a one-to-one function between X and Y. If the interval over which X is defined can be partitioned into k mutually disjoint sets such that each of the inverse functions:
\begin{equation*}
    x_1 = w_1(y), \hspace{1cm} ..., x_k = w_k(y)
\end{equation*}

Then the probability distribution of Y is:
\begin{equation*}
    g(y) = \sum_{i=1}^lf[w_i(y)]|J_i|
\end{equation*}

If we have distribution Z = X + Y, the distribution of Z, $h(z)$, is:

\begin{equation*}
    P(Z) = P(X + Y = z) = \sum_wP(X = w)P(Y = z-w)
\end{equation*}
\begin{align*}
    h(z) &= \int_{-\infty}^{\infty}f(w)g(z-w)dw \\
    &= \sum_{w =-\infty}^{\infty}f(w)g(z-w)dw
\end{align*}

\subsection{Moments and Moment-Generating Functions}

The \textbf{r$^{th}$ moment about the origin} of RV X, given $g(X) = X^r, r \in \mathbb{Z}$ is defined as:

\begin{equation*}
    \mu'_r = E(X^r) = \begin{cases}
        \sum_xx^rf(x), \hspace{0.5cm} \text{discrete case}\\
        \int_{-\infty}^{\infty}x^rf(x)dx, \hspace{0.5cm} \text{continuous case}
    \end{cases}
\end{equation*}

with $\mu$ = $\mu'_1$ and $\sigma^2$ = $\mu'_2 - \mu^2$.

The \textbf{moment-generating function}, $M_X(t)$, is defined as:

\begin{equation*}
    M_X(t) = E(e^{tX}) = \begin{cases}
        \sum_xe^{tx}f(x), \hspace{0.5cm} \text{discrete}\\
        \int_{-\infty}^{\infty}e^{tx}f(x)dx, \hspace{0.5cm} \text{continuous}
    \end{cases}
\end{equation*}

Moment-generating functions will exist only if the sum or integral above converges. If a moment-generating function of a random variable X does exist,
it can be used to generate all the moments of that variable:

\begin{equation*}
    \frac{d^rM_X(t)}{dt^r}\Big|_{t=0} = \mu'_r
\end{equation*}

If X is normal with mean $\mu$ and variance $\sigma^2$:
\begin{equation*}
    M_X(t) = e^{\mu t+\frac{t^2\sigma^2}{2}}
\end{equation*}

\subsection*{Linear Combinations of RVs}

If RV X has distribution $f(x)$, what is distribution, $g(y)$, given Y = aX:
\begin{itemize}
    \item \textbf{Discrete:} $h(y) = f(y/a)$
    \item \textbf{Continuous:} $h(y) = \frac{1}{|a|}f(y/a)$
\end{itemize}

Suppose the same X has moment-generating function $M_X(t)$:

\begin{equation*}
    M_Y(t) = M_X(at)
\end{equation*}

Similarly:
\begin{equation*}
    M_{aX}(t) = M_X(at)
\end{equation*}

With prior example of Z = X + Y, the moment-generating function for Z is:

\begin{equation*}
    M_Z(t) = M_X(t)M_Y(t)
\end{equation*}

\vfill\pagebreak

\section{Sampling}

We sample because we can't always take the data of the whole population.

\begin{itemize}
    \item \textbf{Sample data:} {$x_1, ..., x_n$}, where each data point is a \textbf{realization} of a RV, $X_i$.
    \item \textbf{Sample mean:}
    \begin{align*}
        \bar{x} &= \frac{1}{n}\sum_{i=1}^nx_i\\
        \bar{X} &= \frac{1}{n}\sum_{i=1}^nX_i
    \end{align*}
    \item \textbf{Sample median:} midpoint of sample data set, meaning the data set must be \textbf{ordered}.
    \begin{equation*}
        \text{median} = \begin{cases}
            \frac{1}{2}(x_{\frac{n}{2}} + x_{\frac{n}{2}+1}), \hspace{0.5cm} \text{for n even}\\
            x_{\frac{n+1}{2}}, \hspace{2cm} \text{for n odd}
        \end{cases}
    \end{equation*}
    \item \textbf{Sample mode:} most commonly occurring value
\end{itemize}

\subsection{Measures of variability}
\begin{itemize}
    \item \textbf{Sample variance:}
    \begin{equation*}
        s^2 = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2
    \end{equation*}
    \item \textbf{Sample std. dev.:} $\sqrt{s^2} = s$
    \item (n - 1)\footnote[4]{See pg. 71-72 on Taylor's notes for proof.} is referred to as the \textbf{degrees of freedom}.
\end{itemize}

\subsection{Visualization}
\subsubsection*{Histogram}
\begin{itemize}
    \item Uses a relative frequency distribution: 
    \begin{equation*}
        \text{relative f} = \frac{\text{frequency}}{\text{total occurrences}}
    \end{equation*}
    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{pics/histogram.png}
    \end{Figure}
\end{itemize}
\subsubsection*{Box-and-whisker Plot}
\begin{itemize}
    \item The box plot encloses the \textbf{interquartile range ---} between the 25th and 75th percentile \textbf{---} in a box.
    \item At the extremes, the whiskers show the extreme observations.
    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{pics/box plot.png}
    \end{Figure}
\end{itemize}

\section{Random Sampling}

\begin{itemize}
    \item A \textbf{population} is all possible observations; each observation is a realization of a RV.
    \item A \textbf{sample} is a subset of a population.
\end{itemize}

A \textbf{random sample} with n observations in the sample. Observation i is the realization of independent RV $X_i$ where i = 1, ..., n.

\begin{equation*}
    f(x_1 ... x_n) = f(x_1)f(x_2)...f(x_n)
\end{equation*}
\begin{itemize}
    \item A \textbf{statistic} is a function of the $X_i$, such as mean, median, mode.
    \item A sample is biased if it consistently over/underestimates the statistic of interest.
\end{itemize}
\subsection{Sampling Distribution}
The probability distribution of the statistic is called a sampling distribution.

Facts about random sampling:
\begin{itemize}
    \item If $X_1$ and $X_2$ are normal with means $\mu_{1,2}$ and variances $\sigma^2_{1,2}$, the distribution of $X_1$ + $X_2$ is normal with mean $\mu_1 + \mu_2$ and variance $\sigma^2_1+ \sigma^2_2$.
    \item If X is normal with $\mu$ and $\sigma^2$, then X/n has $\mu$/n and $\sigma^2/n^2$.
    \item If $X_1...X_n$ are normal (mean $\mu$, variance $\sigma^2$), then $\bar{X}$ has $\mu$ and $\sigma^2$/n.
\end{itemize}

\subsection{Central Limit Theorem}

Given the sample $X_1 ... X_n$, which are realizations of IID RVs, the sample average is $\bar{X_n}$. Let:

\begin{equation*}
    Z_n = \frac{\bar{X_n} - \mu}{\sigma/\sqrt{n}}
\end{equation*}

\begin{itemize}
    \item The \textbf{Central Limit Theorem} states that as n $\rightarrow \infty$, the distribution of $Z_n$ converges to the standard normal distribution n(z; 0, 1).
    \item The normal approximation for $\bar{X}$ is good for $n \geq 30$; should only use approximation for $n < 30$ if population is not too different from a normal distribution.
    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{pics/CLT.png}
    \end{Figure}
    \item Note that the standard deviation of $\bar{X}$ is $\sigma/\sqrt{n}$. Also, as n increases, the variance of the distribution decreases at a rate of $\sqrt{n}$.
\end{itemize}
\subsubsection*{Sample Variance Distribution}
\begin{itemize}
    \item Recall: 
    \begin{equation*}
        S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2
    \end{equation*}
    
    \item Let
    \begin{equation*}
        \chi^2 = \frac{(n-1)S^2}{\sigma^2} = \frac{1}{\sigma^2}\sum_{i=1}^n(X_i-\bar{X})^2
    \end{equation*}
    
\end{itemize}

Then $\chi^2$ has a chi-squared distribution with $v = n-1$, where v is known as the \textbf{degrees of freedom} (no. of independent pieces of information).
\begin{itemize}
    \item Suppose a known population mean $\mu$:

    \begin{equation*}
        \frac{1}{\sigma^2}\sum_{i=1}^n(X_i - \mu)^2
    \end{equation*}
    \item This is a chi-squared distribution with $v = n$, one less degree of freedom because one DOF is lost when estimating $\mu$.

\end{itemize}

\subsection{t-Distribution}

\begin{itemize}
    \item We use a t-distribution when the variance of the population is not known.
    \item Consider the statistic:
    \begin{equation*}
        T = \frac{\bar{X} - \mu}{S/\sqrt{n}}
    \end{equation*}
    with
    \begin{equation*}
        S = \sqrt{\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2}
    \end{equation*}
    \item Note that if $n \geq 30$, S is close to $\sigma$ and T follows normal, but if $n < 30$, the \textbf{t-distribution is much more accurate.} 
    \item We can also write the statistic as:
    \begin{align*}
        T &= \frac{(\bar{X}-\mu)/\frac{\sigma}{\sqrt{n}}}{\frac{S}{\sigma}}\\
        &= \frac{Z}{\sqrt{\frac{V}{n-1}}}
    \end{align*}
    where Z has the standard normal distribution and V has chi-squared distribution with $n-1$ DOF.
    \item t-distribution with $v$ DOF is given by:
    \begin{equation*}
        h(t) = \frac{\Gamma[\frac{(v+1)}{2}]}{\Gamma[\frac{v}{2}]\sqrt{\pi v}}(1+\frac{t^2}{v})^{-\frac{v+1}{2}}
    \end{equation*}
    \item We expect more variability with a t-distribution because we do not know $\sigma$ exactly; instead we use S as an estimate.
    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{pics/t dist.png}
    \end{Figure}
    \item The t-distribution is used for problems that deal with inference about the population mean ($\mu$) or in problems that involve comparative samples. It requires that $X_1 ... X_n$ be normal.
    \item It does not relate to Central Limit Theorem.
\end{itemize}

\subsection{Quantile}

\begin{itemize}
    \item Given a sample $x_1 ... x_n$, the \textbf{quantile}, $q(f)$, is value for which a specified fraction f of the data values is less than or equal to q(f).
    \item \textbf{Quantile plot:} $q(f)$ versus f.
    \begin{equation*}
        f_i = \frac{i - \frac{3}{8}}{n + \frac{1}{4}}
    \end{equation*}
    \item To sketch, for each data point $i = 1 ... n$, plot $f_i$ for every $x_i$ (where the sample data is in increasing order).
    \item \textbf{Sample median ---} $q(0.5)$; \textbf{lower quartile ---} $q(0.25)$, 25th percentile; \textbf{upper quartile ---} $q(0.75)$, 75th percentile.
    \item Flat regions \textbf{---} data clusters; steep regions \textbf{---} data sparsity.    
\end{itemize}

The quantile function has a close relation with the CDF:

\begin{itemize}
    \item CDF F(x) is the chance that outcome is less or equal to x \textbf{---} $P(X \leq x)$.
    \item If F is continuous and strictly increasing, $q = F^{-1}(x)$ \textbf{---} swap axes on graph.
\end{itemize}

\subsubsection*{Normal Quantile-Quantile Plot}

The normal quantile-quantile plot takes advantage of what is known about the quantiles of the normal distribution. The methodology involves a plot of the empirical quantiles recently discussed against the corresponding quantile of the normal distribution. Now, the expression for a quantile of an N($\mu$, $\sigma$) random variable is
very complicated. However, a good approximation is given by:

\begin{equation*}
    q_{\mu,\sigma}(f) = \mu + \sigma\{4.91[f^{0.14}-(1-f)^{0.14}]\}
\end{equation*}

\begin{itemize}
    \item If the curve of the normal QQ plot is straight, the data is roughly normal.
    \item Y-intercept is an estimate of population mean ($\mu$), slope is an estimate of standard deviation ($\sigma$).
\end{itemize}

\section{Estimation}

\begin{itemize}
    \item \textbf{Unbiased Estimator} $\longrightarrow \mu_{\theta} = E[\theta] = \theta$
    \item Given P($\theta_L < \theta < \theta_U$) = 1 - $\alpha$, $\mathbb{\theta \in (\theta_L, \theta_U)}$ is a (1 - $\alpha$ confidence interval.
    \item Estimating mean: $\bar{x}$ will be an accurate estimate of $\mu$ for large n.
    \begin{equation*}
        P(-z_{\alpha/2}< Z < z_{\alpha/2}) = 1 - \alpha
    \end{equation*}
    for Z = $\frac{\bar{X} - \mu}{\sigma/\sqrt{n}}$.
    \begin{Figure}
        \includegraphics[width=\linewidth]{pics/conf int.jpg}
    \end{Figure}
    \item To estimate mean using a confidence interval:
    \begin{equation*}
        P(\bar{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} < \mu < \bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n }}) = 1 - \alpha 
    \end{equation*}
    \item If $\bar{X}$ is used to estimate $\mu$, we can be (1-$\alpha$)(100\%) confident that the error will not exceed $z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$.
    \item If $\bar{X}$ is used to estimate $\mu$, we can be (1-$\alpha$)(100\%) confident that the error will not exceed error \textbf{e} when sample size n = $(\frac{z_{\alpha/2}\sigma}{e})^2$.
    \item \textbf{One-Sided Confidence Intervals:} same idea as two-sided, except only 1 error bound: $\theta_U = \bar{x} + z_{\alpha}\frac{\sigma}{\sqrt{n}}, \theta_L = \bar{x} - z_{\alpha}\frac{\sigma}{\sqrt{n}}$.
    \item When the $\sigma$ is unknown, it is the same idea to estimate the $\mu$, but with a T-distribution instead of a normal distribution.
    \begin{equation*}
         P(\bar{X} - t_{\alpha/2}\frac{S}{\sqrt{n}} < \mu < \bar{X} + t_{\alpha/2}\frac{S}{\sqrt{n }}) = 1 - \alpha 
    \end{equation*}
\end{itemize}
\subsection{Prediction Intervals}
\begin{itemize}
    \item accounting for the variation of a future observation.
    \begin{equation*}
        z = \frac{x_0 - \bar{x}}{\sigma\sqrt{1 + \frac{1}{n}}}
    \end{equation*}
    \item The prediction interval is as follows:
    \begin{equation*}
        \bar{x} - z_{\alpha/2}\sigma\sqrt{1 + \frac{1}{n}} < x_0 < \bar{x} + z_{\alpha/2}\sigma\sqrt{1+\frac{1}{n}}
    \end{equation*}
    where $z_{\alpha/2} = -\Phi^{-1}(\alpha/2)$
    \item If the $x_0$ falls out of the range of the prediction interval, it can be considered an \textbf{outlier}.
\end{itemize}
\subsection{Tolerance Limits}
\begin{itemize}
    \item Third type of confidence interval, concerned about long-range performance
    \item In a sample with $\mu$ and $\sigma$, the \textbf{tolerance interval} for the middle 95\% observations of the population is $\mu \pm 1.96\sigma$.
    \item Instead, a (1 - $\gamma$)100\% confidence can be asserted that the given limits contain at least the proportion 1 - $\alpha$ of the measurements.
    \item Choose interval $x\pm ks$ such that 100(1 - $\alpha$)\% of population is within limit.
\end{itemize}

\subsection{Two Samples}
\begin{itemize}
    \item Now we want to estimate $\mu_1 - \mu_2$.
    \item We know that the sampling distribution is normal with mean $\mu_1 - \mu_2$ and variance $\sigma_1^2/n_1 + \sigma_2^2/n_2$.
    \item As such we define RV Z as:
    \begin{equation*}
        Z = \frac{\bar{X_1} - \bar{X_2} - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+ \frac{\sigma_2^2}{n_2}}}
    \end{equation*}
    \item By CLT, the distribution is approximately standard normal --- n(x;0,1)
\end{itemize}
\subsubsection{Unknown Variance (Equal)}

\begin{itemize}
    \item unknown variance < 30.
    \begin{equation*}
        T = \frac{(\bar{x_1} - \bar{x_2}) - \mu_1-\mu_2}{s_p\sqrt{1/n_1 + 1/n_2}}
    \end{equation*}
    \item Pooled estimate of variance ($S_p`^2$):
    \begin{equation*}
        S_p^2 = \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}
    \end{equation*}
\end{itemize}

\subsubsection{Unknown Variance (Different)}

\begin{itemize}
    \item If $\sigma_1 \neq \sigma_2$, use statistic:
    \begin{equation*}
        T' = \frac{\bar{X_1} - \bar{X_2} - (\mu_1 - \mu_2)}{\sqrt{\frac{S_1^2}{n_1}+ \frac{S_2^2}{n_2}}}
    \end{equation*}
    \begin{Figure}
        \includegraphics[width=\linewidth]{pics/idk.png}
    \end{Figure}
\end{itemize}

\subsection{Paired Observations}
\begin{itemize}
    \item Estimation procedure for the difference of 2 means when the samples are not independent and the variances are not necessarily equal.
    \item Rather, each homogeneous experimental unit receives both population conditions; as a result, each experimental unit has a pair. The two populations are “before” and “after,” and the experimental unit is the individual.
    The setup is as follows:
    \item Paired samples ($X_i, Y_i$), i = 1, ..., n; we are interested in their \textbf{difference, $D_i$,} $X_i - Y_i$.
    \item Then, their variance is as follows:
    \begin{equation*}
        var(D_i) = var(X_i - Y_i) = \sigma^2_X + \sigma^2_Y - 2cov(X_i,Y_i)
    \end{equation*}
    \item Helpful variance reduction (take advantage when possible)
    \item Apply usual CLT/t-dist confidence intervals to sample $D_i$:
    \begin{equation*}
        P(-t_{\alpha/2} < T < t_{\alpha/2}) = 1 - \alpha
    \end{equation*}
    where T = $\frac{\bar{D}-\mu_D}{S_d/\sqrt{n}}$ and $t_{\alpha/2}$ is a value of the t-dist with n-1 DOF.
\end{itemize}
\subsection{Estimating a Proportion}

\begin{itemize}
    \item Point estimator of proportion p in binomial experiment (Bernoulli process) is given by statistic $\hat{P} = \frac{X}{n}$ --- X is no. of successes in n trials.
    \item By CLT (large n), $\hat{P}$ is approximately distributed normally w mean and variance:
    \begin{equation*}
        \mu_P = E[\hat{P}] = p; \sigma_{\hat{P}}^2 = \frac{pq}{n}
    \end{equation*}
    \begin{equation*}
        P(-z_{\alpha/2} < Z < z_{\alpha/2}) = 1 - \alpha
    \end{equation*}
    with Z = $\frac{\hat{P}-p}{\sqrt{pq/n}}$.
    \item If n is large, replace p with $\hat{p} = \frac{x}{n}$ in denominator:
    \begin{equation*}
        1 - \alpha = P\big(\hat{P} - z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \leq p \leq \hat{P} + z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}})
    \end{equation*}
    \item If n is small, solve for p with this fuckery:
    \begin{Figure}
        \includegraphics[width=\linewidth]{pics/what.png}
    \end{Figure}
    \item Same with confidence intervals, if we want to be 100(1 - $\alpha$)\% sure that the error does not exceed e, we use sample size:
    \begin{equation*}
        n = \frac{z_{\alpha/2}^2\hat{p}(1-\hat{p})}{e^2} \geq \frac{z_{\alpha/2}^2}{4e^2}
    \end{equation*}
\end{itemize}
\subsection{Estimating the Variance}

\begin{itemize}
    \item An interval estimate of $\sigma^2$ can be established using the statistic:
    \begin{equation*}
        W^2 = \frac{(n-1)S^2}{\sigma^2}
    \end{equation*}
    which has a chi-squared distribution.
    \item Chi-squared distribution is not symmetric, cannot apply the same logic to determine a confidence interval.
    \begin{equation*}
        P(\chi^2_{1-\alpha/2} \leq \frac{(n-1)S^2}{\sigma^2} \leq \chi^2_{\alpha/2}) = 1 - \alpha
    \end{equation*}
    \begin{Figure}
        \includegraphics[width=\linewidth]{pics/chi sq.png}
    \end{Figure}
    \item To determine a confidence interval for $\sigma^2$:
    \begin{equation*}
        P(\frac{(n-1)S^2}{\chi^2_{\alpha/2}} \leq \sigma^2 \leq \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}}) = 1 - \alpha
    \end{equation*}
\end{itemize}

\subsection{Maximum Likelihood Estimation}

\begin{itemize}
    \item Method of maximum likelihood is to maximize the likelihood function.
    \item \textbf{Likelihood Function:} RVs $X_1 ... X_n$, parameter $\theta$, JPDF $f(x_1, ..., x_n; \theta)$
    \begin{align*}
        L(x_1, ..., x_n; \theta) &= f(x_1, ..., x_n; \theta)\\
        &= f(x_1;\theta)...f(x_n;\theta) \\
        &= \prod_{i=1}^n f(x_i;\theta)
    \end{align*}
    \item the philosophy of maximum likelihood estimation evolves from the notion that the reasonable estimator of a parameter based on sample information is that parameter value that produces the largest probability of obtaining the sample.
\end{itemize}

\textbf{Example:} Poisson Distribution
\begin{itemize}
    \item Poisson PMF:
    \begin{equation*}
        f(x|\mu) = \frac{e^{-\mu}\mu^x}{x!}
    \end{equation*}
    \item Likelihood Function:
    \begin{equation*}
        L(x_1, ..., x_n;\mu) = \prod_{i=1}^n f(x_i;\mu) = \frac{e^{-n\mu}\mu^{\sum_{i=1}^n x_i}}{\prod_{i=1}^nx_i!}
    \end{equation*}
    \item Taking the logarithm, we can get rid of the product (log of a product becomes a sum).
    \begin{equation*}
        \ln{L(x_1, ..., x_n;\mu)} = -n\mu + \sum_{i=1}^n x_i\ln{\mu} - \ln{\prod_{i=1}^nx_i!}
    \end{equation*}
    \item Maximize likelihood function by taking derivative wrt $\mu$:
    \begin{equation*}
        \frac{\partial\ln{L(x_1, ..., x_n;\mu)}}{\partial\mu} = -n + \sum_{i=1}^n\frac{x_i}{\mu}
    \end{equation*}
    \item Solve for $\hat{\mu}$ by setting derivative to 0:
    \begin{equation*}
        \hat{\mu} = \sum_{i=1}^n\frac{x_i}{n} = \bar{x}
    \end{equation*}
    \item Since $\mu$ is the mean of the Poisson distribution, the sample average would certainly seem like a reasonable estimator.
    \item \textbf{Important property:}
    \begin{equation*}
        \prod_{i=1}^ne^{x_i} = e^{\sum_{i=1}^nx_i}
    \end{equation*}
\end{itemize}

\vfill\pagebreak

\section{Hypotheses}

\begin{itemize}
     \item \textbf{Statistic Hypothesis:} assertion or conjecture concerning one or more populations.
     \item \textbf{Role of Probability in Hypothesis Testing:} decision procedure must include an awareness of the probability of a wrong conclusion; rejection of a hypothesis implies that the sample evidence refutes it.
\end{itemize}

\textbf{Example:}

A sample of 100 revealing 20 defective items is certainly evidence for rejection. Why? If, indeed, p = 0.10, the probability of obtaining 20 or more defectives is approximately 0.002. With the resulting small risk of a wrong conclusion, it would seem safe to reject the hypothesis that p = 0.10.

\subsection{Null and Alternative Hypotheses}
\begin{itemize}
    \item \textbf{Null Hypothesis:} any hypothesis we wish to test and is denoted by $H_0$.
    \item In turn, the rejection of $H_0$ implies the acceptance of an \textbf{alternative hypothesis}, $H_1$, which represents the question to answer.
    \item Should arrive at one of two conclusions:
    \begin{enumerate}
        \item reject $H_0$ in favor of $H_1$ bc sufficient evidence
        \item fail to reject $H_0$ bc of insufficient evidence
    \end{enumerate}
\end{itemize}

\subsection{Error}

\begin{itemize}
    \item Rejection of the null hypothesis when it is true is called a \textbf{type I error}.
    \begin{itemize}
        \item \textbf{example:} $H_0$ states $\mu = 68$, probability of type I error is:
        \begin{equation*}
            \alpha = P(\mu < 67) + P(\mu > 69)
        \end{equation*}
        \item if n > 30, assume normally distributed, use CLT centered around $\mu = 68$.
    \end{itemize}
    \item Nonrejection of the null hypothesis when it is false is called a \textbf{type II error}.
    \begin{itemize}
        \item \textbf{same example:} it is only necessary to
consider the probability of not rejecting $H_0$ that $\mu$ = 68 when the alternative $\mu$ = 70 is true. 
        \item A type II error will result when the sample mean $\bar{x}$ falls between 67 and 69 when H1 is true.
        \begin{equation*}
            \beta = P(67 \leq \mu \leq 69)
        \end{equation*}
        centered around $\mu = 70$.
    \end{itemize}
    \begin{Figure}
        \includegraphics[width=\linewidth]{pics/type 2.png}
    \end{Figure}
    \item The probability of committing a type I error, denoted by $\alpha$, is called \textbf{the level of significance}.
    \item The probability of committing a type II error, denoted by $\beta$, is impossible to compute unless we have a specific alternative hypothesis.
    \item Probability of committing both types of error can be reduced by increasing the sample size.
    \item The probability of committing a type II error increases rapidly when the true value of $\mu$ approaches, but is not equal to, the hypothesized value.
    \begin{itemize}
        \item \textbf{still that example:} if the alternative hypothesis $\mu = 68.5$, we don't mind coming to the conclusion that $\mu = 68$.
    \end{itemize}
    \begin{Figure}
        \includegraphics[width=\linewidth]{pics/t2 erro.png}
    \end{Figure}
    \item \textbf{power of a test:} defined as 1 - $\beta$
\end{itemize}

\subsubsection{One or Two-Tailed Tests}
\begin{itemize}
    \item \textbf{One-sided:} $H_0: \theta = \theta_0; H_1: \theta <$ or $> \theta_0$.
    \item \textbf{Two-sided:} $H_0: \theta = \theta_0; H_1: \theta \neq \theta_0$.
\end{itemize}

\subsection{P-Values}

In testing hypotheses in which the test statistic is discrete, the critical region may be chosen arbitrarily and its size determined. If $\alpha$ is too large, it can be reduced by making an adjustment in the critical value.
\begin{itemize}
    \item \textbf{P-value:} lowest level (of significance) at which the observed value of the test statistic is significant.
\end{itemize}

\subsubsection{P-Value Approach}
\begin{enumerate}
    \item State null and alternative hypotheses.
    \item Choose an appropriate test statistic.
    \item Compute the P-value based on the computed value of the test statistic.
    \item Use judgment based on the P-value and knowledge of the scientific system.
\end{enumerate}

\subsection{Single Mean}

\subsubsection{Variance Known (Z-Distribution)}

\begin{itemize}
    \item Simply put, non-rejection region for $H_0$ is:
    \begin{equation*}
        z \in [-z_{\alpha/2}, z_{\alpha/2}]
    \end{equation*}
    \begin{Figure}
        \includegraphics[width=\linewidth]{pics/critical region.png}
    \end{Figure}
    \item \textbf{P-Value Calculation:}
    \begin{align*}
        P &= P(\Big|\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}\Big| > |z|) \\
        &= P(|Z| > |z|)\\
        &= 2P(Z > |z|)
    \end{align*}
\end{itemize}

\subsubsection{Variance Unknown (T-Distribution)}
\begin{itemize}
    \item Same idea as above but use t-distribution instead of CLT.
\end{itemize}

\subsection{Two Samples: Two Means}

\begin{itemize}
    \item Writing the null and alternative hypotheses:
    \begin{center}
        $H_0: \mu_1 - \mu_2 = d_0$\\
        $H_1: \mu_1 - \mu_2 \neq d_0$
    \end{center}
    \item Use statistic z:
    \begin{equation*}
        z = \frac{(\bar{x_1} - \bar{x_2} - d_0)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}}
    \end{equation*}
    \item Non-rejection region is still $z \in [-z_{\alpha/2}, z_{\alpha/2}]$ for two-tailed tests.
    \item For one-tailed test, use upper or lower limit of z depending on the direction of the less/greater than sign in the alternative hypothesis.
\end{itemize}

\subsubsection{Unknown, Equal Variances}

\begin{itemize}
    \item Use pooled t-test, recall $S_p^2$:
    \begin{align*}
        t &= \frac{(\bar{x_1} - \bar{x_2}) - d_0}{s_p\sqrt{1/n_1 + 1/n_2}}\\
        s_p^2 &= \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
    \end{align*}
    \item Non-rejection range of $H_0$ is:
    \begin{equation*}
        t \in [-t_{\alpha/2, n_1+n_2-2}, t_{\alpha/2, n_1+n_2-2}]
    \end{equation*}
\end{itemize}
\subsubsection{Unknown, Non-equal Variances}

\begin{itemize}
    \item Use T': just look at Section 10.3.2 I'm too lazy to rewrite the formulas.
    \item Non-rejection range of $H_0$ is:
    \begin{equation*}
        t' \in [-t_{\alpha/2, v}, t_{\alpha/2, v}]
    \end{equation*}
\end{itemize}

\subsubsection{Paired Observations}

Just look down I don't even get this, just know that $\bar{d}$ is the same idea as section 10.4.

\end{multicols}
\subsection{Choice of Sample Size}

\begin{figure}[!ht]
    \includegraphics[width=\linewidth]{pics/table.png}
\end{figure}
\begin{multicols}{2}

\subsubsection{One Sample}
    \begin{itemize}
        \item For a specific alternative hypothesis, say $\mu =\mu_0 + \delta$, the power of the test is:
        \begin{align*}
            1 - \beta &= P(\bar{X} > a \text{ when } \mu = \mu_0 +\delta) \\
            \beta &= P(\bar{X} < a \text{ when } \mu = \mu_0 +\delta)\\
            \beta &= P\Big[\frac{\bar{X} - (\mu_0+\delta)}{\sigma/\sqrt{n}}<\frac{a - (\mu_0+\delta)}{\sigma/\sqrt{n}}\big]
        \end{align*}
        \item Take this as a normal distribution with $\mu = \mu_0 +\delta$:
        \begin{align*}
            \beta &= P(Z < z_{\alpha} - \frac{\delta}{\sigma/\sqrt{n}})\\ 
            -z_{\beta} &= z_{\alpha} - \frac{\delta\sqrt{n}}{\sigma}
        \end{align*}
        \item We can then conclude that we choose the sample size to be for a \textbf{one-sided test} (works both ways):
        \begin{equation*}
            n = \frac{(z_{\alpha} + z_{\beta})^2\sigma^2}{\delta^2}
        \end{equation*}
        \item For a \textbf{two-sided test}:
        \begin{equation*}
            n \approx \frac{(z_{\alpha/2} + z_{\beta})^2\sigma^2}{\delta^2}
        \end{equation*}
    \end{itemize}

\subsubsection{Two Samples}

\begin{itemize}
    \item Same idea as one sample: new Z is defined as this:
    \begin{equation*}
        Z = \frac{\bar{X_1} - \bar{X_2} - (d_0 + \delta)}{\sqrt{(\sigma_1^2 + \sigma_2^2)/n}}
    \end{equation*}
    \item We can conclude that for \textbf{two-tailed test:}
    \begin{equation*}
        n \approx \frac{(z_{\alpha/2} + z_{\beta})^2(\sigma_1^2+\sigma_2^2)}{\delta^2}
    \end{equation*}
    \item \textbf{One-tailed:}
    \begin{equation*}
         n = \frac{(z_{\alpha} + z_{\beta})^2(\sigma_1^2+\sigma_2^2)}{\delta^2}
    \end{equation*}
\end{itemize}

\subsection{Variances: One and Two Samples}

\begin{itemize}
    \item Writing the null and alternative hypotheses:
        \begin{center}
        $H_0: \sigma^2 = \sigma_0^2$\\
        $H_1: \sigma^2 \neq \sigma_0^2$
    \end{center}
    \item Use chi-squared statistic to base our decision to reject $H_0$ or not:
    \begin{equation*}
        \chi^2 = \frac{(n-1)s^2}{\sigma_0^2}
    \end{equation*}
    \item Non-rejection region ($\alpha$, \textbf{two-tailed}) of $H_0$:
    \begin{equation*}
        \chi^2 \in [\chi^2_{1-\alpha/2}, \chi^2_{\alpha/2}]
    \end{equation*}
    \item Non-rejection region ($\alpha$, \textbf{one-tailed}, $\sigma^2 < \sigma_0^2$) of $H_0$:
    \begin{equation*}
        \chi^2 \geq \chi^2_{1-\alpha}
    \end{equation*}
    \item Non-rejection region ($\alpha$, \textbf{one-tailed}, $\sigma^2 > \sigma^2_0$) of $H_0$:
    \begin{equation*}
        \chi^2 \leq \chi^2_{\alpha}
    \end{equation*}
    
\end{itemize}

\end{multicols}

\end{document}
